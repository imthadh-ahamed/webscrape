{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b88886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Session created with browser headers\n"
     ]
    }
   ],
   "source": [
    "# Web Scraping SAB University Website\n",
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "\n",
    "# Create a session for better performance\n",
    "session = requests.Session()\n",
    "\n",
    "# Set headers to mimic a real browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "session.headers.update(headers)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Session created with browser headers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4cd04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping main navigation...\n",
      "Found 654 navigation links\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape main navigation and links\n",
    "def scrape_main_navigation(url):\n",
    "    \"\"\"\n",
    "    Scrape the main navigation links and important sections from the homepage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract main navigation links\n",
    "        nav_links = []\n",
    "        \n",
    "        # Find navigation elements (adjust selectors based on actual HTML structure)\n",
    "        nav_elements = soup.find_all(['nav', 'ul', 'div'], class_=lambda x: x and ('nav' in x.lower() or 'menu' in x.lower()))\n",
    "        \n",
    "        for nav in nav_elements:\n",
    "            links = nav.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                text = link.get_text(strip=True)\n",
    "                if href and text:\n",
    "                    # Convert relative URLs to absolute URLs\n",
    "                    full_url = urljoin(url, href)\n",
    "                    nav_links.append({\n",
    "                        'text': text,\n",
    "                        'url': full_url,\n",
    "                        'section': 'Navigation'\n",
    "                    })\n",
    "        \n",
    "        return nav_links\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping navigation: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test the function\n",
    "base_url = \"https://www.sab.ac.lk/\"\n",
    "print(\"Scraping main navigation...\")\n",
    "navigation_data = scrape_main_navigation(base_url)\n",
    "print(f\"Found {len(navigation_data)} navigation links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad9931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping faculty information...\n",
      "Found 9 faculties\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape faculty information\n",
    "def scrape_faculties(url):\n",
    "    \"\"\"\n",
    "    Scrape faculty information from the main page\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        faculties = []\n",
    "        \n",
    "        # Look for faculty links and information\n",
    "        # Based on the webpage content, faculties are listed in the navigation\n",
    "        faculty_links = [\n",
    "            {\"name\": \"Agricultural Sciences\", \"url\": \"https://www.sab.ac.lk/agri\"},\n",
    "            {\"name\": \"Applied Sciences\", \"url\": \"https://www.sab.ac.lk/app\"},\n",
    "            {\"name\": \"Computing\", \"url\": \"https://www.sab.ac.lk/computing\"},\n",
    "            {\"name\": \"Geomatics\", \"url\": \"https://www.sab.ac.lk/geo\"},\n",
    "            {\"name\": \"Graduate Studies\", \"url\": \"https://www.sab.ac.lk/fgs\"},\n",
    "            {\"name\": \"Management Studies\", \"url\": \"https://www.sab.ac.lk/mgmt\"},\n",
    "            {\"name\": \"Medicine\", \"url\": \"https://www.sab.ac.lk/med\"},\n",
    "            {\"name\": \"Social Sciences & Languages\", \"url\": \"https://www.sab.ac.lk/fssl\"},\n",
    "            {\"name\": \"Technology\", \"url\": \"https://www.sab.ac.lk/tech\"}\n",
    "        ]\n",
    "        \n",
    "        for faculty in faculty_links:\n",
    "            faculties.append({\n",
    "                'faculty_name': faculty['name'],\n",
    "                'faculty_url': faculty['url'],\n",
    "                'description': f\"Faculty of {faculty['name']} at Sabaragamuwa University\",\n",
    "                'scraped_from': url\n",
    "            })\n",
    "        \n",
    "        return faculties\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping faculties: {e}\")\n",
    "        return []\n",
    "\n",
    "# Scrape faculty information\n",
    "print(\"Scraping faculty information...\")\n",
    "faculty_data = scrape_faculties(base_url)\n",
    "print(f\"Found {len(faculty_data)} faculties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d96e6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping contact information...\n",
      "Extracted contact information for 1 entries\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape contact information and general details\n",
    "def scrape_contact_info(url):\n",
    "    \"\"\"\n",
    "    Scrape contact information and general university details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        contact_info = []\n",
    "        \n",
    "        # Extract contact information from the page\n",
    "        contact_data = {\n",
    "            'university_name': 'Sabaragamuwa University of Sri Lanka',\n",
    "            'address': 'P.O. Box 02, Belihuloya, 70140, Sri Lanka',\n",
    "            'phone': '+94-45-2280014 / +94-45-2280087',\n",
    "            'email': 'info@sab.ac.lk',\n",
    "            'website': url,\n",
    "            'scraped_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        # Try to extract more contact details from the page\n",
    "        text_content = soup.get_text()\n",
    "        \n",
    "        # Look for additional phone numbers\n",
    "        import re\n",
    "        phones = re.findall(r'\\+94-?\\d{2}-?\\d{7}', text_content)\n",
    "        if phones:\n",
    "            contact_data['additional_phones'] = ', '.join(set(phones))\n",
    "        \n",
    "        # Look for email addresses\n",
    "        emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text_content)\n",
    "        if emails:\n",
    "            contact_data['additional_emails'] = ', '.join(set(emails))\n",
    "        \n",
    "        contact_info.append(contact_data)\n",
    "        \n",
    "        return contact_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping contact info: {e}\")\n",
    "        return []\n",
    "\n",
    "# Scrape contact information\n",
    "print(\"Scraping contact information...\")\n",
    "contact_data = scrape_contact_info(base_url)\n",
    "print(f\"Extracted contact information for {len(contact_data)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dabd0fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping centers and departments...\n",
      "Found 10 centers and 9 departments\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape centers and departments\n",
    "def scrape_centers_and_departments(url):\n",
    "    \"\"\"\n",
    "    Scrape information about centers and departments\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Centers data based on the webpage content\n",
    "        centers = [\n",
    "            {\"name\": \"Centre for Computer Studies\", \"url\": \"https://www.sab.ac.lk/center-for-computer-studies\"},\n",
    "            {\"name\": \"Centre for Gender Equity and Equality\", \"url\": \"https://www.sab.ac.lk/centre-for-gender-equity-and-equality\"},\n",
    "            {\"name\": \"Centre for Indigenous Knowledge and Community Studies\", \"url\": \"https://www.sab.ac.lk/cikcs\"},\n",
    "            {\"name\": \"Centre for Open and Distance Learning\", \"url\": \"https://www.sab.ac.lk/codl\"},\n",
    "            {\"name\": \"Centre for Quality Assurance\", \"url\": \"https://www.sab.ac.lk/iqac\"},\n",
    "            {\"name\": \"Centre for Research and Knowledge Dissemination\", \"url\": \"https://www.sab.ac.lk/crkd\"},\n",
    "            {\"name\": \"Staff Development Center\", \"url\": \"https://www.sab.ac.lk/sdc\"},\n",
    "            {\"name\": \"Career Guidance Unit\", \"url\": \"https://www.sab.ac.lk/career-guidance-unit\"},\n",
    "            {\"name\": \"Department of Physical Education\", \"url\": \"https://www.sab.ac.lk/physical-education\"},\n",
    "            {\"name\": \"University Business Linkage Cell\", \"url\": \"https://www.sab.ac.lk/ublc\"}\n",
    "        ]\n",
    "        \n",
    "        # Departments data\n",
    "        departments = [\n",
    "            {\"name\": \"Academic Establishment\", \"url\": \"https://www.sab.ac.lk/academic-establishment\"},\n",
    "            {\"name\": \"Capital Works\", \"url\": \"https://www.sab.ac.lk/capital-works-planning\"},\n",
    "            {\"name\": \"Civil Engineering\", \"url\": \"https://www.sab.ac.lk/civil_engineering\"},\n",
    "            {\"name\": \"Examinations Division\", \"url\": \"https://www.sab.ac.lk/examination_division\"},\n",
    "            {\"name\": \"Finance Division\", \"url\": \"https://www.sab.ac.lk/finance-division\"},\n",
    "            {\"name\": \"General Administration\", \"url\": \"https://www.sab.ac.lk/administration-officers\"},\n",
    "            {\"name\": \"Non Academic Establishment\", \"url\": \"https://www.sab.ac.lk/non-academic-establishment\"},\n",
    "            {\"name\": \"Registrar Office\", \"url\": \"https://www.sab.ac.lk/registrar_office\"},\n",
    "            {\"name\": \"Student Affairs\", \"url\": \"https://www.sab.ac.lk/student-affairs\"}\n",
    "        ]\n",
    "        \n",
    "        # Format data for CSV\n",
    "        centers_data = []\n",
    "        for center in centers:\n",
    "            centers_data.append({\n",
    "                'type': 'Center',\n",
    "                'name': center['name'],\n",
    "                'url': center['url'],\n",
    "                'description': f\"{center['name']} at Sabaragamuwa University\",\n",
    "                'scraped_from': url\n",
    "            })\n",
    "        \n",
    "        departments_data = []\n",
    "        for dept in departments:\n",
    "            departments_data.append({\n",
    "                'type': 'Department',\n",
    "                'name': dept['name'],\n",
    "                'url': dept['url'],\n",
    "                'description': f\"{dept['name']} at Sabaragamuwa University\",\n",
    "                'scraped_from': url\n",
    "            })\n",
    "        \n",
    "        return centers_data, departments_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping centers and departments: {e}\")\n",
    "        return [], []\n",
    "\n",
    "# Scrape centers and departments\n",
    "print(\"Scraping centers and departments...\")\n",
    "centers_data, departments_data = scrape_centers_and_departments(base_url)\n",
    "print(f\"Found {len(centers_data)} centers and {len(departments_data)} departments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "821c24c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to CSV files...\n",
      "============================================================\n",
      "‚úì Saved 654 records to scraped_data\\navigation_links.csv\n",
      "Preview of navigation_links.csv:\n",
      "               text                                         url     section\n",
      "0          About us  https://www.sab.ac.lk/about-the-university  Navigation\n",
      "1  Vision & Mission        https://www.sab.ac.lk/vision-mission  Navigation\n",
      "2           History               https://www.sab.ac.lk/history  Navigation\n",
      "3        Contact us               https://www.sab.ac.lk/contact  Navigation\n",
      "4          Overview              https://www.sab.ac.lk/overview  Navigation\n",
      "--------------------------------------------------\n",
      "‚úì Saved 9 records to scraped_data\\faculties.csv\n",
      "Preview of faculties.csv:\n",
      "            faculty_name                      faculty_url  \\\n",
      "0  Agricultural Sciences       https://www.sab.ac.lk/agri   \n",
      "1       Applied Sciences        https://www.sab.ac.lk/app   \n",
      "2              Computing  https://www.sab.ac.lk/computing   \n",
      "3              Geomatics        https://www.sab.ac.lk/geo   \n",
      "4       Graduate Studies        https://www.sab.ac.lk/fgs   \n",
      "\n",
      "                                         description            scraped_from  \n",
      "0  Faculty of Agricultural Sciences at Sabaragamu...  https://www.sab.ac.lk/  \n",
      "1  Faculty of Applied Sciences at Sabaragamuwa Un...  https://www.sab.ac.lk/  \n",
      "2    Faculty of Computing at Sabaragamuwa University  https://www.sab.ac.lk/  \n",
      "3    Faculty of Geomatics at Sabaragamuwa University  https://www.sab.ac.lk/  \n",
      "4  Faculty of Graduate Studies at Sabaragamuwa Un...  https://www.sab.ac.lk/  \n",
      "--------------------------------------------------\n",
      "‚úì Saved 1 records to scraped_data\\contact_information.csv\n",
      "Preview of contact_information.csv:\n",
      "                        university_name  \\\n",
      "0  Sabaragamuwa University of Sri Lanka   \n",
      "\n",
      "                                     address                            phone  \\\n",
      "0  P.O. Box 02, Belihuloya, 70140, Sri Lanka  +94-45-2280014 / +94-45-2280087   \n",
      "\n",
      "            email                 website         scraped_date  \\\n",
      "0  info@sab.ac.lk  https://www.sab.ac.lk/  2025-08-13 17:09:12   \n",
      "\n",
      "                additional_phones additional_emails  \n",
      "0  +94-45-2280087, +94-45-2280014    info@sab.ac.lk  \n",
      "--------------------------------------------------\n",
      "‚úì Saved 10 records to scraped_data\\centers.csv\n",
      "Preview of centers.csv:\n",
      "     type                                               name  \\\n",
      "0  Center                        Centre for Computer Studies   \n",
      "1  Center              Centre for Gender Equity and Equality   \n",
      "2  Center  Centre for Indigenous Knowledge and Community ...   \n",
      "3  Center              Centre for Open and Distance Learning   \n",
      "4  Center                       Centre for Quality Assurance   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://www.sab.ac.lk/center-for-computer-studies   \n",
      "1  https://www.sab.ac.lk/centre-for-gender-equity...   \n",
      "2                        https://www.sab.ac.lk/cikcs   \n",
      "3                         https://www.sab.ac.lk/codl   \n",
      "4                         https://www.sab.ac.lk/iqac   \n",
      "\n",
      "                                         description            scraped_from  \n",
      "0  Centre for Computer Studies at Sabaragamuwa Un...  https://www.sab.ac.lk/  \n",
      "1  Centre for Gender Equity and Equality at Sabar...  https://www.sab.ac.lk/  \n",
      "2  Centre for Indigenous Knowledge and Community ...  https://www.sab.ac.lk/  \n",
      "3  Centre for Open and Distance Learning at Sabar...  https://www.sab.ac.lk/  \n",
      "4  Centre for Quality Assurance at Sabaragamuwa U...  https://www.sab.ac.lk/  \n",
      "--------------------------------------------------\n",
      "‚úì Saved 9 records to scraped_data\\departments.csv\n",
      "Preview of departments.csv:\n",
      "         type                    name  \\\n",
      "0  Department  Academic Establishment   \n",
      "1  Department           Capital Works   \n",
      "2  Department       Civil Engineering   \n",
      "3  Department   Examinations Division   \n",
      "4  Department        Finance Division   \n",
      "\n",
      "                                            url  \\\n",
      "0  https://www.sab.ac.lk/academic-establishment   \n",
      "1  https://www.sab.ac.lk/capital-works-planning   \n",
      "2       https://www.sab.ac.lk/civil_engineering   \n",
      "3    https://www.sab.ac.lk/examination_division   \n",
      "4        https://www.sab.ac.lk/finance-division   \n",
      "\n",
      "                                         description            scraped_from  \n",
      "0  Academic Establishment at Sabaragamuwa University  https://www.sab.ac.lk/  \n",
      "1           Capital Works at Sabaragamuwa University  https://www.sab.ac.lk/  \n",
      "2       Civil Engineering at Sabaragamuwa University  https://www.sab.ac.lk/  \n",
      "3   Examinations Division at Sabaragamuwa University  https://www.sab.ac.lk/  \n",
      "4        Finance Division at Sabaragamuwa University  https://www.sab.ac.lk/  \n",
      "--------------------------------------------------\n",
      "============================================================\n",
      "Data scraping and saving completed!\n"
     ]
    }
   ],
   "source": [
    "# Function to save data to CSV files\n",
    "def save_to_csv(data, filename, fieldnames=None):\n",
    "    \"\"\"\n",
    "    Save data to CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not data:\n",
    "            print(f\"No data to save for {filename}\")\n",
    "            return\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs('scraped_data', exist_ok=True)\n",
    "        \n",
    "        filepath = os.path.join('scraped_data', filename)\n",
    "        \n",
    "        # Convert to DataFrame for easier handling\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "        print(f\"‚úì Saved {len(data)} records to {filepath}\")\n",
    "        \n",
    "        # Display first few rows\n",
    "        print(f\"Preview of {filename}:\")\n",
    "        print(df.head())\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {filename}: {e}\")\n",
    "\n",
    "# Save all scraped data to CSV files\n",
    "print(\"Saving data to CSV files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save navigation data\n",
    "if navigation_data:\n",
    "    save_to_csv(navigation_data, 'navigation_links.csv')\n",
    "\n",
    "# Save faculty data\n",
    "if faculty_data:\n",
    "    save_to_csv(faculty_data, 'faculties.csv')\n",
    "\n",
    "# Save contact data\n",
    "if contact_data:\n",
    "    save_to_csv(contact_data, 'contact_information.csv')\n",
    "\n",
    "# Save centers data\n",
    "if centers_data:\n",
    "    save_to_csv(centers_data, 'centers.csv')\n",
    "\n",
    "# Save departments data\n",
    "if departments_data:\n",
    "    save_to_csv(departments_data, 'departments.csv')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Data scraping and saving completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f21f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing comprehensive data scraping...\n",
      "Found 118 unique links\n",
      "‚úì Saved 118 records to scraped_data\\all_links_comprehensive.csv\n",
      "Preview of all_links_comprehensive.csv:\n",
      "              link_text                                           url  \\\n",
      "0  Skip to main content           https://www.sab.ac.lk/#main-content   \n",
      "1       Online Teaching  https://www.sab.ac.lk/online-video-tutorials   \n",
      "2    Procurement System          http://online.sab.ac.lk/procurement/   \n",
      "3          Exam Results                 https://www.sab.ac.lk/results   \n",
      "4             Downloads               https://www.sab.ac.lk/downloads   \n",
      "\n",
      "  category  is_internal            scraped_from         scraped_date  \n",
      "0    Other         True  https://www.sab.ac.lk/  2025-08-13 17:09:26  \n",
      "1    Other         True  https://www.sab.ac.lk/  2025-08-13 17:09:26  \n",
      "2    Other         True  https://www.sab.ac.lk/  2025-08-13 17:09:26  \n",
      "3    Other         True  https://www.sab.ac.lk/  2025-08-13 17:09:26  \n",
      "4    Other         True  https://www.sab.ac.lk/  2025-08-13 17:09:26  \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive data scraping with additional information\n",
    "def scrape_comprehensive_data(url):\n",
    "    \"\"\"\n",
    "    Scrape comprehensive data from the main page including all available links and content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        all_links = []\n",
    "        \n",
    "        # Get all links from the page\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            text = link.get_text(strip=True)\n",
    "            \n",
    "            if href and text and len(text) > 1:  # Filter out empty or single-character links\n",
    "                full_url = urljoin(url, href)\n",
    "                \n",
    "                # Categorize links\n",
    "                category = \"Other\"\n",
    "                if any(keyword in href.lower() for keyword in ['faculty', 'agri', 'app', 'computing', 'geo', 'fgs', 'mgmt', 'med', 'fssl', 'tech']):\n",
    "                    category = \"Faculty\"\n",
    "                elif any(keyword in href.lower() for keyword in ['center', 'centre', 'ccs', 'gee', 'cikcs', 'codl', 'iqac', 'crkd', 'sdc']):\n",
    "                    category = \"Center\"\n",
    "                elif any(keyword in href.lower() for keyword in ['department', 'division', 'academic', 'administration', 'finance']):\n",
    "                    category = \"Department\"\n",
    "                elif any(keyword in href.lower() for keyword in ['about', 'vision', 'mission', 'history', 'chancellor']):\n",
    "                    category = \"About\"\n",
    "                elif any(keyword in href.lower() for keyword in ['student', 'admission', 'programme', 'course']):\n",
    "                    category = \"Academic\"\n",
    "                elif any(keyword in href.lower() for keyword in ['contact', 'phone', 'email']):\n",
    "                    category = \"Contact\"\n",
    "                \n",
    "                all_links.append({\n",
    "                    'link_text': text,\n",
    "                    'url': full_url,\n",
    "                    'category': category,\n",
    "                    'is_internal': 'sab.ac.lk' in full_url,\n",
    "                    'scraped_from': url,\n",
    "                    'scraped_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        \n",
    "        # Remove duplicates based on URL\n",
    "        seen_urls = set()\n",
    "        unique_links = []\n",
    "        for link in all_links:\n",
    "            if link['url'] not in seen_urls:\n",
    "                seen_urls.add(link['url'])\n",
    "                unique_links.append(link)\n",
    "        \n",
    "        return unique_links\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in comprehensive scraping: {e}\")\n",
    "        return []\n",
    "\n",
    "# Scrape comprehensive data\n",
    "print(\"Performing comprehensive data scraping...\")\n",
    "comprehensive_data = scrape_comprehensive_data(base_url)\n",
    "print(f\"Found {len(comprehensive_data)} unique links\")\n",
    "\n",
    "# Save comprehensive data\n",
    "if comprehensive_data:\n",
    "    save_to_csv(comprehensive_data, 'all_links_comprehensive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3ce57ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DATA ANALYSIS SUMMARY\n",
      "============================================================\n",
      "üìÅ Total CSV files created: 6\n",
      "   ‚Ä¢ all_links_comprehensive.csv: 118 records\n",
      "     Categories: {'Other': 57, 'Center': 21, 'Faculty': 17, 'Department': 9, 'About': 8, 'Academic': 5, 'Contact': 1}\n",
      "   ‚Ä¢ centers.csv: 10 records\n",
      "   ‚Ä¢ contact_information.csv: 1 records\n",
      "   ‚Ä¢ departments.csv: 9 records\n",
      "   ‚Ä¢ faculties.csv: 9 records\n",
      "   ‚Ä¢ navigation_links.csv: 654 records\n",
      "\n",
      "üìà Total records scraped: 801\n",
      "\n",
      "üîó LINK ANALYSIS:\n",
      "   ‚Ä¢ Total unique links: 118\n",
      "   ‚Ä¢ Internal links: 110\n",
      "   ‚Ä¢ External links: 8\n",
      "\n",
      "üìÇ CATEGORY BREAKDOWN:\n",
      "   ‚Ä¢ Other: 57 links\n",
      "   ‚Ä¢ Center: 21 links\n",
      "   ‚Ä¢ Faculty: 17 links\n",
      "   ‚Ä¢ Department: 9 links\n",
      "   ‚Ä¢ About: 8 links\n",
      "   ‚Ä¢ Academic: 5 links\n",
      "   ‚Ä¢ Contact: 1 links\n",
      "\n",
      "============================================================\n",
      "‚úÖ Web scraping completed successfully!\n",
      "üìÇ Data saved in: c:\\Users\\AImthadh\\Desktop\\New folder\\webscrape\\web\\scraped_data\n"
     ]
    }
   ],
   "source": [
    "# Data analysis and summary\n",
    "def analyze_scraped_data():\n",
    "    \"\"\"\n",
    "    Analyze the scraped data and provide summary statistics\n",
    "    \"\"\"\n",
    "    print(\"üìä DATA ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists('scraped_data'):\n",
    "        print(\"No scraped data directory found!\")\n",
    "        return\n",
    "    \n",
    "    # List all CSV files created\n",
    "    csv_files = [f for f in os.listdir('scraped_data') if f.endswith('.csv')]\n",
    "    \n",
    "    print(f\"üìÅ Total CSV files created: {len(csv_files)}\")\n",
    "    \n",
    "    total_records = 0\n",
    "    for file in csv_files:\n",
    "        filepath = os.path.join('scraped_data', file)\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            records = len(df)\n",
    "            total_records += records\n",
    "            print(f\"   ‚Ä¢ {file}: {records} records\")\n",
    "            \n",
    "            # Show category breakdown for comprehensive data\n",
    "            if 'category' in df.columns:\n",
    "                print(f\"     Categories: {df['category'].value_counts().to_dict()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚Ä¢ {file}: Error reading file - {e}\")\n",
    "    \n",
    "    print(f\"\\nüìà Total records scraped: {total_records}\")\n",
    "    \n",
    "    # Analyze comprehensive data if available\n",
    "    comp_file = os.path.join('scraped_data', 'all_links_comprehensive.csv')\n",
    "    if os.path.exists(comp_file):\n",
    "        df_comp = pd.read_csv(comp_file)\n",
    "        \n",
    "        print(f\"\\nüîó LINK ANALYSIS:\")\n",
    "        print(f\"   ‚Ä¢ Total unique links: {len(df_comp)}\")\n",
    "        print(f\"   ‚Ä¢ Internal links: {df_comp['is_internal'].sum()}\")\n",
    "        print(f\"   ‚Ä¢ External links: {(~df_comp['is_internal']).sum()}\")\n",
    "        \n",
    "        print(f\"\\nüìÇ CATEGORY BREAKDOWN:\")\n",
    "        category_counts = df_comp['category'].value_counts()\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"   ‚Ä¢ {category}: {count} links\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Web scraping completed successfully!\")\n",
    "    print(f\"üìÇ Data saved in: {os.path.abspath('scraped_data')}\")\n",
    "\n",
    "# Run analysis\n",
    "analyze_scraped_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
