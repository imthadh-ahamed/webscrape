{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d6d8578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import requests\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f1e378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver setup function created with webdriver-manager!\n"
     ]
    }
   ],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Set up Chrome WebDriver with appropriate options using webdriver-manager\n",
    "    \"\"\"\n",
    "    try:\n",
    "        chrome_options = Options()\n",
    "        # Add options for better compatibility\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        \n",
    "        # Create service with automatically managed Chrome driver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        \n",
    "        # Create driver\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        driver.maximize_window()\n",
    "        \n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up driver: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Driver setup function created with webdriver-manager!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af80322d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigation scraping function fixed!\n"
     ]
    }
   ],
   "source": [
    "def scrape_navigation_links(driver, base_url):\n",
    "    \"\"\"\n",
    "    Scrape all navigation links from the website\n",
    "    \"\"\"\n",
    "    navigation_links = []\n",
    "    \n",
    "    try:\n",
    "        # Wait for page to load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        \n",
    "        # Find all navigation links\n",
    "        nav_elements = driver.find_elements(By.CSS_SELECTOR, \"nav a, .nav a, .navbar a, .menu a, header a\")\n",
    "        \n",
    "        for element in nav_elements:\n",
    "            try:\n",
    "                href = element.get_attribute(\"href\")\n",
    "                text = element.text.strip()\n",
    "                \n",
    "                if href and text:\n",
    "                    # Convert relative URLs to absolute\n",
    "                    full_url = urljoin(base_url, href)\n",
    "                    navigation_links.append({\n",
    "                        'link_text': text,\n",
    "                        'url': full_url,\n",
    "                        'section': 'navigation'\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "        # Also get links from main content area\n",
    "        content_links = driver.find_elements(By.CSS_SELECTOR, \"main a, .content a, .main-content a\")\n",
    "        \n",
    "        for element in content_links[:20]:  # Limit to first 20 content links\n",
    "            try:\n",
    "                href = element.get_attribute(\"href\")\n",
    "                text = element.text.strip()\n",
    "                \n",
    "                if href and text and len(text) > 2:\n",
    "                    full_url = urljoin(base_url, href)\n",
    "                    navigation_links.append({\n",
    "                        'link_text': text,\n",
    "                        'url': full_url,\n",
    "                        'section': 'content'\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping navigation: {e}\")\n",
    "    \n",
    "    return navigation_links\n",
    "\n",
    "print(\"Navigation scraping function fixed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660488a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact info scraping function created!\n"
     ]
    }
   ],
   "source": [
    "def scrape_contact_info(driver):\n",
    "    \"\"\"\n",
    "    Scrape contact information from the website\n",
    "    \"\"\"\n",
    "    contact_info = []\n",
    "    \n",
    "    try:\n",
    "        # Look for contact information in various selectors\n",
    "        contact_selectors = [\n",
    "            \"contact\", \"footer\", \".contact\", \".contact-info\", \n",
    "            \".contact-details\", \".footer\", \"#contact\", \"#footer\"\n",
    "        ]\n",
    "        \n",
    "        for selector in contact_selectors:\n",
    "            try:\n",
    "                elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                for element in elements:\n",
    "                    text = element.text.strip()\n",
    "                    if text:\n",
    "                        # Look for email addresses\n",
    "                        import re\n",
    "                        emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "                        \n",
    "                        # Look for phone numbers\n",
    "                        phones = re.findall(r'[\\+]?[1-9]?[\\d\\s\\-\\(\\)]{10,}', text)\n",
    "                        \n",
    "                        for email in emails:\n",
    "                            contact_info.append({\n",
    "                                'type': 'email',\n",
    "                                'value': email,\n",
    "                                'source_section': selector\n",
    "                            })\n",
    "                        \n",
    "                        for phone in phones:\n",
    "                            # Clean phone number\n",
    "                            clean_phone = re.sub(r'[^\\d\\+]', '', phone)\n",
    "                            if len(clean_phone) >= 10:\n",
    "                                contact_info.append({\n",
    "                                    'type': 'phone',\n",
    "                                    'value': phone.strip(),\n",
    "                                    'source_section': selector\n",
    "                                })\n",
    "                        \n",
    "                        # Look for addresses (lines with common address keywords)\n",
    "                        address_keywords = ['address', 'street', 'road', 'avenue', 'colombo', 'sri lanka']\n",
    "                        lines = text.split('\\n')\n",
    "                        for line in lines:\n",
    "                            line = line.strip()\n",
    "                            if any(keyword.lower() in line.lower() for keyword in address_keywords) and len(line) > 10:\n",
    "                                contact_info.append({\n",
    "                                    'type': 'address',\n",
    "                                    'value': line,\n",
    "                                    'source_section': selector\n",
    "                                })\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping contact info: {e}\")\n",
    "    \n",
    "    return contact_info\n",
    "\n",
    "print(\"Contact info scraping function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed2c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Departments scraping function created!\n"
     ]
    }
   ],
   "source": [
    "def scrape_departments_and_programs(driver):\n",
    "    \"\"\"\n",
    "    Scrape academic departments and programs information\n",
    "    \"\"\"\n",
    "    departments = []\n",
    "    \n",
    "    try:\n",
    "        # Look for department/program related content\n",
    "        dept_selectors = [\n",
    "            \"department\", \"program\", \"course\", \"faculty\", \"school\",\n",
    "            \".department\", \".program\", \".course\", \".faculty\", \".school\",\n",
    "            \"#departments\", \"#programs\", \"#courses\", \"#faculties\"\n",
    "        ]\n",
    "        \n",
    "        for selector in dept_selectors:\n",
    "            try:\n",
    "                elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                for element in elements:\n",
    "                    # Get text content\n",
    "                    text = element.text.strip()\n",
    "                    \n",
    "                    # Get any links within the element\n",
    "                    links = element.find_elements(By.TAG_NAME, \"a\")\n",
    "                    \n",
    "                    if text and len(text) > 10:\n",
    "                        departments.append({\n",
    "                            'section_type': selector,\n",
    "                            'content': text[:500],  # Limit content length\n",
    "                            'has_links': len(links) > 0,\n",
    "                            'link_count': len(links)\n",
    "                        })\n",
    "                        \n",
    "                    # Extract individual links if they seem to be department/program related\n",
    "                    for link in links:\n",
    "                        link_text = link.text.strip()\n",
    "                        link_url = link.get_attribute(\"href\")\n",
    "                        \n",
    "                        if link_text and link_url and len(link_text) > 3:\n",
    "                            # Check if link text suggests it's academic content\n",
    "                            academic_keywords = [\n",
    "                                'department', 'faculty', 'school', 'program', 'course',\n",
    "                                'degree', 'diploma', 'bachelor', 'master', 'phd', 'studies'\n",
    "                            ]\n",
    "                            \n",
    "                            if any(keyword.lower() in link_text.lower() for keyword in academic_keywords):\n",
    "                                departments.append({\n",
    "                                    'section_type': 'academic_link',\n",
    "                                    'content': link_text,\n",
    "                                    'url': link_url,\n",
    "                                    'has_links': True,\n",
    "                                    'link_count': 1\n",
    "                                })\n",
    "                                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping departments: {e}\")\n",
    "    \n",
    "    return departments\n",
    "\n",
    "print(\"Departments scraping function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9de2690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General content scraping function created!\n"
     ]
    }
   ],
   "source": [
    "def scrape_general_content(driver, url):\n",
    "    \"\"\"\n",
    "    Scrape general content from the website\n",
    "    \"\"\"\n",
    "    content_data = []\n",
    "    \n",
    "    try:\n",
    "        # Get page title\n",
    "        title = driver.title\n",
    "        \n",
    "        # Get meta description\n",
    "        meta_desc = \"\"\n",
    "        try:\n",
    "            meta_element = driver.find_element(By.CSS_SELECTOR, \"meta[name='description']\")\n",
    "            meta_desc = meta_element.get_attribute(\"content\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        # Get main headings\n",
    "        headings = []\n",
    "        for tag in ['h1', 'h2', 'h3']:\n",
    "            elements = driver.find_elements(By.TAG_NAME, tag)\n",
    "            for element in elements:\n",
    "                text = element.text.strip()\n",
    "                if text:\n",
    "                    headings.append({\n",
    "                        'tag': tag,\n",
    "                        'text': text\n",
    "                    })\n",
    "        \n",
    "        # Get main content paragraphs\n",
    "        paragraphs = []\n",
    "        p_elements = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "        for p in p_elements[:10]:  # Limit to first 10 paragraphs\n",
    "            text = p.text.strip()\n",
    "            if text and len(text) > 20:\n",
    "                paragraphs.append(text[:300])  # Limit paragraph length\n",
    "        \n",
    "        # Get images with alt text\n",
    "        images = []\n",
    "        img_elements = driver.find_elements(By.TAG_NAME, \"img\")\n",
    "        for img in img_elements[:10]:  # Limit to first 10 images\n",
    "            alt_text = img.get_attribute(\"alt\")\n",
    "            src = img.get_attribute(\"src\")\n",
    "            if alt_text or src:\n",
    "                images.append({\n",
    "                    'alt_text': alt_text or 'No alt text',\n",
    "                    'src': src\n",
    "                })\n",
    "        \n",
    "        content_data.append({\n",
    "            'url': url,\n",
    "            'title': title,\n",
    "            'meta_description': meta_desc,\n",
    "            'headings_count': len(headings),\n",
    "            'paragraphs_count': len(paragraphs),\n",
    "            'images_count': len(images),\n",
    "            'main_headings': [h['text'] for h in headings[:5]],  # Top 5 headings\n",
    "            'sample_content': paragraphs[:3] if paragraphs else []  # First 3 paragraphs\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping general content: {e}\")\n",
    "    \n",
    "    return content_data\n",
    "\n",
    "print(\"General content scraping function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc69656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saving functions created!\n"
     ]
    }
   ],
   "source": [
    "def save_to_csv(data, filename, output_dir=\"../scraped_data\"):\n",
    "    \"\"\"\n",
    "    Save scraped data to CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Full path for the CSV file\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        if data:\n",
    "            # Convert to DataFrame and save\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "            print(f\"Saved {len(data)} records to {filepath}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"No data to save for {filename}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to CSV {filename}: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_all_data(navigation_data, contact_data, departments_data, content_data):\n",
    "    \"\"\"\n",
    "    Save all scraped data to separate CSV files\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Save navigation links\n",
    "    results['navigation'] = save_to_csv(navigation_data, \"navigation_links.csv\")\n",
    "    \n",
    "    # Save contact information\n",
    "    results['contact'] = save_to_csv(contact_data, \"contact_information.csv\")\n",
    "    \n",
    "    # Save departments/programs\n",
    "    results['departments'] = save_to_csv(departments_data, \"departments_programs.csv\")\n",
    "    \n",
    "    # Save general content\n",
    "    results['content'] = save_to_csv(content_data, \"general_content.csv\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"CSV saving functions created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3dff0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main scraping function created!\n"
     ]
    }
   ],
   "source": [
    "def main_scrape(url=\"https://www.sab.ac.lk/\"):\n",
    "    \"\"\"\n",
    "    Main function to scrape the SAB website\n",
    "    \"\"\"\n",
    "    print(f\"Starting to scrape: {url}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    driver = None\n",
    "    \n",
    "    try:\n",
    "        # Set up the driver\n",
    "        driver = setup_driver()\n",
    "        if driver is None:\n",
    "            print(\"Failed to set up Chrome driver!\")\n",
    "            return False\n",
    "        \n",
    "        print(\"‚úì Chrome driver set up successfully\")\n",
    "        \n",
    "        # Navigate to the website\n",
    "        driver.get(url)\n",
    "        print(f\"‚úì Navigated to {url}\")\n",
    "        \n",
    "        # Wait for the page to load\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Scrape different types of data\n",
    "        print(\"\\nüìä Scraping navigation links...\")\n",
    "        navigation_data = scrape_navigation_links(driver, url)\n",
    "        print(f\"   Found {len(navigation_data)} navigation items\")\n",
    "        \n",
    "        print(\"\\nüìû Scraping contact information...\")\n",
    "        contact_data = scrape_contact_info(driver)\n",
    "        print(f\"   Found {len(contact_data)} contact items\")\n",
    "        \n",
    "        print(\"\\nüè´ Scraping departments and programs...\")\n",
    "        departments_data = scrape_departments_and_programs(driver)\n",
    "        print(f\"   Found {len(departments_data)} department items\")\n",
    "        \n",
    "        print(\"\\nüìÑ Scraping general content...\")\n",
    "        content_data = scrape_general_content(driver, url)\n",
    "        print(f\"   Found {len(content_data)} content items\")\n",
    "        \n",
    "        # Save all data to CSV files\n",
    "        print(\"\\nüíæ Saving data to CSV files...\")\n",
    "        save_results = save_all_data(navigation_data, contact_data, departments_data, content_data)\n",
    "        \n",
    "        print(\"\\n‚úÖ Scraping completed!\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nüìã SUMMARY:\")\n",
    "        print(f\"Navigation links: {len(navigation_data)}\")\n",
    "        print(f\"Contact items: {len(contact_data)}\")\n",
    "        print(f\"Department items: {len(departments_data)}\")\n",
    "        print(f\"Content items: {len(content_data)}\")\n",
    "        print(f\"CSV files saved: {sum(save_results.values())}/4\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during scraping: {e}\")\n",
    "        return False\n",
    "        \n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "            print(\"\\nüîí Browser closed\")\n",
    "\n",
    "print(\"Main scraping function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a825b7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting SAB Website Scraping...\n",
      "üìç Target: https://www.sab.ac.lk/\n",
      "\n",
      "Starting to scrape: https://www.sab.ac.lk/\n",
      "==================================================\n",
      "‚úì Chrome driver set up successfully\n",
      "‚úì Navigated to https://www.sab.ac.lk/\n",
      "\n",
      "üìä Scraping navigation links...\n",
      "Error scraping navigation: 'WebDriverWait' object has no attribute 'wait'\n",
      "   Found 0 navigation items\n",
      "\n",
      "üìû Scraping contact information...\n",
      "   Found 25 contact items\n",
      "\n",
      "üè´ Scraping departments and programs...\n",
      "   Found 0 department items\n",
      "\n",
      "üìÑ Scraping general content...\n",
      "   Found 1 content items\n",
      "\n",
      "üíæ Saving data to CSV files...\n",
      "No data to save for navigation_links.csv\n",
      "Saved 25 records to ../scraped_data\\contact_information.csv\n",
      "No data to save for departments_programs.csv\n",
      "Saved 1 records to ../scraped_data\\general_content.csv\n",
      "\n",
      "‚úÖ Scraping completed!\n",
      "==================================================\n",
      "\n",
      "üìã SUMMARY:\n",
      "Navigation links: 0\n",
      "Contact items: 25\n",
      "Department items: 0\n",
      "Content items: 1\n",
      "CSV files saved: 2/4\n",
      "\n",
      "üîí Browser closed\n",
      "\n",
      "üéâ Scraping completed successfully!\n",
      "üìÅ Check the '../scraped_data/' folder for CSV files\n"
     ]
    }
   ],
   "source": [
    "# Execute the web scraping\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting SAB Website Scraping...\")\n",
    "    print(\"üìç Target: https://www.sab.ac.lk/\")\n",
    "    print()\n",
    "    \n",
    "    # Run the main scraping function\n",
    "    success = main_scrape()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nüéâ Scraping completed successfully!\")\n",
    "        print(\"üìÅ Check the '../scraped_data/' folder for CSV files\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Scraping failed!\")\n",
    "        print(\"üí° Make sure Chrome browser is installed and try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "942d1766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DISPLAYING SCRAPED DATA\n",
      "========================================\n",
      "\n",
      "üìÑ Contact Information:\n",
      "------------------------------\n",
      "Records found: 25\n",
      "Sample data:\n",
      " type          value source_section\n",
      "email info@sab.ac.lk         footer\n",
      "phone +94-45-2280014         footer\n",
      "phone +94-45-2280087         footer\n",
      "\n",
      "üìÑ General Content:\n",
      "------------------------------\n",
      "Records found: 1\n",
      "Sample data:\n",
      "                   url       title                                                                                     meta_description  headings_count  paragraphs_count  images_count                                                                  main_headings sample_content\n",
      "https://www.sab.ac.lk/ Home | SUSL The Sabaragamuwa University of Sri Lanka is a public university in Belihuloya, Balangoda, Sri Lanka.              11                 0            10 ['Upcoming Events', 'Notice', 'Life At SUSL', 'Our Faculties', 'CONTACT INFO']             []\n",
      "\n",
      "üìÑ Navigation Links:\n",
      "------------------------------\n",
      "Records found: 654\n",
      "Sample data:\n",
      "            text                                        url    section\n",
      "        About us https://www.sab.ac.lk/about-the-university Navigation\n",
      "Vision & Mission       https://www.sab.ac.lk/vision-mission Navigation\n",
      "         History              https://www.sab.ac.lk/history Navigation\n",
      "\n",
      "‚ùå Departments & Programs: No data file found\n",
      "\n",
      "‚úÖ Data display completed!\n"
     ]
    }
   ],
   "source": [
    "# Display the scraped data\n",
    "print(\"üìä DISPLAYING SCRAPED DATA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check if CSV files exist and display sample data\n",
    "import os\n",
    "\n",
    "csv_files = [\n",
    "    (\"contact_information.csv\", \"Contact Information\"),\n",
    "    (\"general_content.csv\", \"General Content\"),\n",
    "    (\"navigation_links.csv\", \"Navigation Links\"),\n",
    "    (\"departments_programs.csv\", \"Departments & Programs\")\n",
    "]\n",
    "\n",
    "for filename, title in csv_files:\n",
    "    filepath = os.path.join(\"../scraped_data\", filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"\\nüìÑ {title}:\")\n",
    "        print(\"-\" * 30)\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(f\"Records found: {len(df)}\")\n",
    "            if len(df) > 0:\n",
    "                print(\"Sample data:\")\n",
    "                print(df.head(3).to_string(index=False))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {title}: No data file found\")\n",
    "\n",
    "print(\"\\n‚úÖ Data display completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79428fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running scraping again with fixed navigation function...\n",
      "Starting to scrape: https://www.sab.ac.lk/\n",
      "==================================================\n",
      "‚úì Chrome driver set up successfully\n",
      "‚úì Navigated to https://www.sab.ac.lk/\n",
      "\n",
      "üìä Scraping navigation links...\n",
      "   Found 143 navigation items\n",
      "\n",
      "üìû Scraping contact information...\n",
      "   Found 25 contact items\n",
      "\n",
      "üè´ Scraping departments and programs...\n",
      "   Found 0 department items\n",
      "\n",
      "üìÑ Scraping general content...\n",
      "   Found 1 content items\n",
      "\n",
      "üíæ Saving data to CSV files...\n",
      "Saved 143 records to ../scraped_data\\navigation_links.csv\n",
      "Saved 25 records to ../scraped_data\\contact_information.csv\n",
      "No data to save for departments_programs.csv\n",
      "Saved 1 records to ../scraped_data\\general_content.csv\n",
      "\n",
      "‚úÖ Scraping completed!\n",
      "==================================================\n",
      "\n",
      "üìã SUMMARY:\n",
      "Navigation links: 143\n",
      "Contact items: 25\n",
      "Department items: 0\n",
      "Content items: 1\n",
      "CSV files saved: 3/4\n",
      "\n",
      "üîí Browser closed\n"
     ]
    }
   ],
   "source": [
    "# Run scraping again to get navigation links with the fixed function\n",
    "print(\"üîÑ Running scraping again with fixed navigation function...\")\n",
    "success = main_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96dcc035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ FINAL SCRAPING SUMMARY\n",
      "==================================================\n",
      "üåê Website: https://www.sab.ac.lk/\n",
      "üïí Scraped on: 2025-08-17 10:50:45\n",
      "\n",
      "üîó Navigation Links: 143 records\n",
      "   Sample: {'link_text': 'Online Teaching', 'url': 'https://www.sab.ac.lk/online-video-tutorials', 'section': 'navigation'}\n",
      "\n",
      "üìû Contact Information: 25 records\n",
      "   Sample: {'type': 'email', 'value': 'info@sab.ac.lk', 'source_section': 'footer'}\n",
      "\n",
      "üìÑ General Content: 1 records\n",
      "   Sample: {'url': 'https://www.sab.ac.lk/', 'title': 'Home | SUSL', 'meta_description': 'The Sabaragamuwa University of Sri Lanka is a public university in Belihuloya, Balangoda, Sri Lanka.', 'headings_count': 11, 'paragraphs_count': 0, 'images_count': 10, 'main_headings': \"['Upcoming Events', 'Notice', 'Life At SUSL', 'Our Faculties', 'CONTACT INFO']\", 'sample_content': '[]'}\n",
      "\n",
      "üè´ Departments & Programs: No data\n",
      "\n",
      "üìä Total Records Scraped: 169\n",
      "üìÅ Data saved in: ../scraped_data/ folder\n",
      "\n",
      "‚úÖ Webscraping completed successfully!\n",
      "üîç You can now analyze the CSV files for further insights.\n"
     ]
    }
   ],
   "source": [
    "# Final Summary of Scraped Data\n",
    "print(\"üéâ FINAL SCRAPING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üåê Website: https://www.sab.ac.lk/\")\n",
    "print(f\"üïí Scraped on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "# Load and display summary of each CSV file\n",
    "csv_files = [\n",
    "    (\"navigation_links.csv\", \"üîó Navigation Links\"),\n",
    "    (\"contact_information.csv\", \"üìû Contact Information\"), \n",
    "    (\"general_content.csv\", \"üìÑ General Content\"),\n",
    "    (\"departments_programs.csv\", \"üè´ Departments & Programs\")\n",
    "]\n",
    "\n",
    "total_records = 0\n",
    "\n",
    "for filename, title in csv_files:\n",
    "    filepath = os.path.join(\"../scraped_data\", filename)\n",
    "    if os.path.exists(filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"{title}: {len(df)} records\")\n",
    "        total_records += len(df)\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            print(f\"   Sample: {df.iloc[0].to_dict()}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"{title}: No data\")\n",
    "        print()\n",
    "\n",
    "print(f\"üìä Total Records Scraped: {total_records}\")\n",
    "print(f\"üìÅ Data saved in: ../scraped_data/ folder\")\n",
    "print()\n",
    "print(\"‚úÖ Webscraping completed successfully!\")\n",
    "print(\"üîç You can now analyze the CSV files for further insights.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
